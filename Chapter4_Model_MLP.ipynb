{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据姓氏 分类国籍\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建分类器\n",
    "class SurnameMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SurnameMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x_in, softmax = False):\n",
    "        inter_data1 = F.relu(self.fc1(x_in))\n",
    "        pre_vector = self.fc2(inter_data1)\n",
    "        \n",
    "        if softmax:\n",
    "            pre_vector = F.softmax(pre_vector, dim = 1)\n",
    "        return pre_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集设置 继承 原始类 Dataset\n",
    "def SurnameDataSet(Dataset):\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        # 获取 姓的向量\n",
    "        surname_vector = \n",
    "            self.vectorizer.vectorize(row.surname)\n",
    "        # 获取国家的索引\n",
    "        nationality_index = \n",
    "            self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {'x_surname': surname_vector,\n",
    "               'y_nationality': nationality_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理 向量化\n",
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "    # 向量化\n",
    "    def vectorize(self, surname):\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype = float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        surname_vocab = Vocabulary(unk_token = \"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk = False)\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用一个字典来存放所有需要的 数据信息\n",
    "args = Namespace(\n",
    "    # 数据的路径信息\n",
    "    surname_csv = \"surnames_with_splits.csv\",\n",
    "    vecrotizer_file = \"vectorizer.json\",\n",
    "    model_state_file = \"model.path\",\n",
    "    save_dir = \"ch4/surname_mlp\",\n",
    "    # 模型超级参数\n",
    "    hidden_dim = 300,\n",
    "    # 训练超级参数\n",
    "    seed = 1337,\n",
    "    num_epochs = 100,\n",
    "    early_stopping_criteria = 5,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化 各个部分  数据集， 向量化， 分类器，定义损失函数，设置优化器\n",
    "dataset = SurnameDataSet.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "mlp = SurnameMLP(input_dim = len(vectorizer.surname_vocab),\n",
    "                 hidden_dim = args.hidden_dim,\n",
    "                 output_dim = len(vectorizer.nationality_vocab))\n",
    "mlp = mlp.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr = args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "# 梯度归零\n",
    "optimizer.zero_grad()\n",
    "# 获取 前向传播结果\n",
    "y_pred = mlp(batich_dict['x_surname'])\n",
    "# 计算损失\n",
    "loss = loss_func(y_pred, batich_dict['y_nationality'])\n",
    "loss_batch = loss.to(\"cpu\").item()\n",
    "running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "# 后向传播 使用损失 计算梯度\n",
    "loss.backward()\n",
    "# 梯度迭代\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "def pre_nation(name, mlp, vectorizer):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    result = mlp(vectorized_name, softmax = True)\n",
    "    probability_values, indices = result.max(dim = 1)\n",
    "    index = indices.item()\n",
    "    predicted_nation = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "    return {'nationality': predicted_nationality,\n",
    "           'probability': probability_value}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
